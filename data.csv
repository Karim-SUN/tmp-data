Paper_id,Passive_vs_active,Explanation,Global_vs_local,Paper_title
craven1996etsro,1,4,3,Extracting Tree-Structured Representations of Trained Networks
krishnan1999edtft,1,4,3,Extracting decision trees from trained neural networks
boz2002edtft,1,4,3,Extracting Decision Trees from Trained Neural Networks
johansson2009edtuo,1,4,3,Evolving decision trees using oracle guides
craven1994usaqt,1,4,3,Using sampling and queries to extract rules from trained neural networks
johansson2003reftn,1,4,3,Rule extraction from trained neural networks using genetic programming
zhou2003esrft,1,4,3,Extracting symbolic rules from trained neural network ensembles
augasta2012retnn,1,4,3,Reverse engineering the neural networks for rule extraction in classification problems
henelius2014apitb,1,2,3,A peek into the black box: exploring classifiers by randomization
krishnan2017pmlef,1,4,3,Palm: Machine learning explanations for iterative debugging
zien2009tfirm,1,2,3,The feature importance ranking measure
vidovic2016fimfn-1,1,2,3,Feature importance measure for non-linear learning algorithms
vidovic2016fimfn-2,1,2,1,Feature importance measure for non-linear learning algorithms
xu2015saatn,1,2,1,Show attend and tell: Neural image caption generation with visual attention
fong2017ieobb,1,2,1,Interpretable explanations of black boxes by meaningful perturbation
zhou2016ldffd,1,2,1,Learning deep features for discriminative localization
selvaraju2017gcvef,1,2,1,Grad-cam: Visual explanations from deep networks via gradient-based localization
simonyan2013dicnv,1,2,1,Deep inside convolutional networks: Visualising image classification models and saliency maps
bach2015opwef,1,2,2,On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation
sturm2016idnnf,1,2,1,Interpretable deep neural networks for single-trial EEG classification
montavon2017encdw,1,2,1,Explaining nonlinear classification decisions with deep taylor decomposition
shrikumar2017liftp,1,2,2,Learning Important Features Through Propagating Activation Differences
landecker2013idcoh,1,2,1,Interpreting individual classifications of hierarchical networks
zintgraf2017vdnnd,1,2,1,Visualizing deep neural network decisions: Prediction difference analysis
bojarski2016vvcfa,1,2,1,Visualbackprop: visualizing cnns for autonomous driving
lei2016rnp,1,2,1,Rationalizing neural predictions
strumbelj2010aeeoi,1,2,1,An efficient explanation of individual classifications using game theory
ribeiro2016wsity,1,2,1,Why should i trust you?: Explaining the predictions of any classifier
turner2016ames,1,4,1,A model explanation system
ribeiro2018ahpma,1,4,2,Anchors: High-precision model-agnostic explanations
singh2016pabbe,1,4,1,Programs as black-box explanations
guidotti2018lrbeo,1,4,1,Local rule-based explanations of black box decision systems
haufe2014otiow,1,2,1,On the interpretation of weight vectors of linear models in multivariate neuroimaging
olden2002itbba,1,2,3,Illuminating the ``black box'': a randomization approach for understanding variable contributions in artificial neural networks
baehrens2010hteic,1,2,1,How to explain individual classification decisions
datta2016atvqi-1,1,2,3,Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems
datta2016atvqi-2,1,2,2,Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems
datta2016atvqi-3,1,2,1,Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems
sundararajan2017aafdn,1,2,2,Axiomatic attribution for deep networks
cortez2011obbdm,1,2,1,Opening black box data mining models using sensitivity analysis
yosinski2015unntd,1,3,3,Understanding Neural Networks Through Deep Visualization
zeiler2014vaucn,1,3,3,Visualizing and understanding convolutional networks
springenberg2014sfsta,1,3,3,Striving for simplicity: The all convolutional net
nguyen2016stpif,1,3,3,Synthesizing the preferred inputs for neurons in neural networks via deep generator networks
mahendran2015udirb,1,3,3,Understanding deep image representations by inverting them
radford2017ltgra,1,3,3,Learning to generate reviews and discovering sentiment
wu2018bstro,2,4,3,Beyond Sparsity: Tree Regularization of Deep Models for Interpretability
zhang2018icnn,2,3,3,Interpretable Convolutional Neural Networks
li2019dlfcb,2,1,3,Deep Learning for Case-Based Reasoning through Prototypes: A Neural Network that Explains Its Predictions
chen2019tlltd,2,1,3,This looks like that: deep learning for interpretable image recognition
koh2017ubbpv,1,1,1,Understanding black-box predictions via influence functions
yeh2018rpsfe,1,1,1,Representer Point Selection for Explaining Deep Neural Networks
wang2019gfolc,1,4,2,Gaining Free or Low-Cost Interpretability with Interpretable Partial Substitute
odajima2008grgfd,1,4,3,Greedy rule generation from discrete data and its use in neural network rule extraction
nayak2009grwpt,1,4,3,Generating rules with predicates terms and variables from the pruned neural networks
castro2002ioann,1,4,3,Interpretation of artificial neural networks by means of fuzzy rules
dhurandhar2018ebotm,1,4,1,Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives
wang2018vdnnb,1,3,3,Visualizing deep neural network by alternately image blurring and deblurring
fong2018nqaeh,1,3,3,Net2Vec: Quantifying and Explaining How Concepts Are Encoded by Filters in Deep Neural Networks
dalvi2019wiogo,1,3,3,What is one grain of sand in the desert? analyzing individual neurons in deep nlp models
plumb2018masle,1,2,1,Model agnostic supervised local explanations
lundberg2017auati,1,2,1,A Unified Approach to Interpreting Model Predictions
chen2018lteai,1,2,1,Learning to Explain: An Information-Theoretic Perspective on Model Interpretation
kim2018ibfaq,1,2,3,Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)
wang2019bamba,1,2,1,Bias Also Matters: Bias Attribution for Deep Neural Network Explanation
ancona2019ednnw,1,2,1,Explaining Deep Neural Networks with a Polynomial Time Algorithm for Shapley Value Approximation
goyal2019cve,1,2,1,Counterfactual Visual Explanations
lapuschkin2019uchpa,1,2,3,Unmasking clever hans predictors and assessing what machines really learn
wu2020rtrfi,2,4,2,Regional Tree Regularization for Interpretability in Deep Neural Networks
pedapati2020lgtmc,1,4,3,Learning Global Transparent Models Consistent with Local Contrastive Explanations
heskes2020csvec,1,2,1,Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models
wojtas2020firfd,2,2,3,Feature Importance Ranking for Deep Learning
natesan2020mame-1,1,2,2,Model Agnostic Multilevel Explanations
natesan2020mame-2,1,2,3,Model Agnostic Multilevel Explanations
plumb2020rbbmf,2,2,1,Regularizing Black-box Models for Improved Interpretability
weinberger2020ldapb,2,2,1,Learning Deep Attribution Priors Based On Prior Knowledge
ghorbani2019tacbe,1,2,3,Towards automatic concept-based explanations
